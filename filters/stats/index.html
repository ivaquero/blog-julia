<!DOCTYPE html><pre class="language-julia"><code>module stats
# implements some common statistical functions
export mahalanobis, log_pdf, log_likelihood, likelihood
export gaussian, mul, add, covariance_ellipse
export Normal, MvNormal, pdf, mean, var, cov

using Statistics
using Distributions
using LinearAlgebra


FloatOrArray = Union{Real,AbstractArray}

&quot;&quot;&quot;
    mahalanobis(x, y, cov)

Returns the mahalanobis distance between `x` and `y`

# Examples
```julia-repl
julia> mahalanobis(3., 3.5, 4.0^2) # univariate case
0.125

julia> mahalanobis(3., 6., 1.) # univariate, 3 std away
3.0

julia> mahalanobis([1., 2], [1.1, 3.5], [1. .1; .1 13])
0.4253332705891392
```
&quot;&quot;&quot;
function mahalanobis(x::FloatOrArray, y::FloatOrArray, cov::FloatOrArray)
    size(x) == size(y) || throw(DimensionMismatch(&quot;size of x not equal to size of y&quot;))
    dist = sqrt((y .- x)&apos; * inv(cov) * (y .- x))
    return dist
end

&quot;&quot;&quot;
    log_pdf(d::Distribution, x::Union{Real, AbstractArray{Real}})

Computes the log of the probability density function of the normal distribution
N(mean, cov) for the data x. The normal may be univariate or multivariate.
&quot;&quot;&quot;
log_pdf(d::Distribution, x::FloatOrArray) = Distributions.logpdf(d, x)

&quot;&quot;&quot;
    log_likelihood(z, x, P, H, R)

Returns log-likelihood of the measurement z given the Gaussian
posterior (x, P) using measurement function H and measurement
covariance error R
&quot;&quot;&quot;
log_likelihood(z::Real, x::Real, P::Real, H::Real, R::Real) = log_pdf(Normal(H * x, sqrt(H * P * H + R)), z)
log_likelihood(z::AbstractArray, x::AbstractArray, P::AbstractArray, H::AbstractArray, R::AbstractArray) = log_pdf(MvNormal(H * x, H * P * H&apos; + R), z)

&quot;&quot;&quot;
    likelihood(z, x, P, H, R)

Returns likelihood of the measurement z given the Gaussian
posterior (x, P) using measurement function H and measurement
covariance error R
&quot;&quot;&quot;
likelihood(z, x, P, H, R) = exp(log_likelihood(z, x, P, H, R))

&quot;&quot;&quot;
    gaussian(x, mean, var)

Returns un-normalized probability density function (pdf) for x given a Gaussian with the
specified mean and variance.

For univariate distribution

    •   If `x` is a scalar, it returns a scalar

    •   If `x` is a vector, it returns a vector with pdf at given points

For multivariate distribution

    •   If `x` is a vector, it returns the result as a scalar.

    •   If `x` is a matrix with n columns, it returns a vector `r` of length n, where
        `r[i]` corresponds to `x[:,i]` (i.e. treating each column as a sample).

&quot;&quot;&quot;
gaussian(x::Real, mean::Real, var::Real) = pdf(Normal(mean, sqrt(var)), x)
gaussian(x::AbstractArray, mean::Real, var::Real) = pdf.(Normal(mean, sqrt(var)), x)
gaussian(x::AbstractArray, mean::AbstractArray, cov::AbstractArray) = pdf(MvNormal(mean, cov), x)

&quot;&quot;&quot;
    mul(d1:: Distribution, d2:: Distribution)

Returns the product of the two gaussian distributions
&quot;&quot;&quot;
function mul(d1::UnivariateDistribution, d2::UnivariateDistribution)
    mean1 = Statistics.mean(d1)
    mean2 = Statistics.mean(d2)

    size(mean1) == size(mean2) || throw(DimensionMismatch(&quot;size of distributions must be equal&quot;))

    var1 = Statistics.var(d1)
    var2 = Statistics.var(d2)

    var3 = 1.0 / (1.0 / var1 + 1.0 / var2)
    mean3 = (var1 * mean2 + var2 * mean1) / (var1 + var2)

    return Normal(mean3, sqrt(var3))
end

function mul(d1::MultivariateDistribution, d2::MultivariateDistribution)
    mean1 = Statistics.mean(d1)
    mean2 = Statistics.mean(d2)

    size(mean1) == size(mean2) || throw(DimensionMismatch(&quot;size of distributions must be equal&quot;))

    var1 = Statistics.cov(d1)
    var2 = Statistics.cov(d2)

    sum_inv = inv(var1 + var2)
    var3 = var1 * sum_inv * var2
    mean3 = var2 * sum_inv * mean1 + var1 * sum_inv * mean2

    return MvNormal(mean3, var3)
end

&quot;&quot;&quot;
    add(d1::Distribution, d2::Distribution)

Returns the sum of the two gaussian distributions
&quot;&quot;&quot;
function add(d1::UnivariateDistribution, d2::UnivariateDistribution)
    mean1 = Statistics.mean(d1)
    mean2 = Statistics.mean(d2)
    size(mean1) == size(mean2) || throw(DimensionMismatch(&quot;size of distributions must be equal&quot;))
    var1 = Statistics.var(d1)
    var2 = Statistics.var(d2)

    mean3 = mean1 + mean2
    var3 = var1 + var2

    return Normal(mean3, sqrt(var3))
end

function add(d1::MultivariateDistribution, d2::MultivariateDistribution)
    mean1 = Statistics.mean(d1)
    mean2 = Statistics.mean(d2)
    size(mean1) == size(mean2) || throw(DimensionMismatch(&quot;size of distributions must be equal&quot;))
    var1 = Statistics.cov(d1)
    var2 = Statistics.cov(d2)

    mean3 = mean1 .+ mean2
    var3 = var1 .+ var2

    return MvNormal(mean3, var3)
end

&quot;&quot;&quot;
    covariance_ellipse(P, deviations=1.0)

Returns a tuple (angle, width, height) defining the covariance ellipse
representing the 2 dimensional covariance matrix P
&quot;&quot;&quot;
function covariance_ellipse(P::AbstractArray; deviations::Real=1.0)
    size(P) == (2, 2) || throw(DimensionMismatch(&quot;P must be a 2x2 matrix&quot;))
    U, s, _ = svd(P)
    orientation = atan(U[2, 1], U[1, 1])
    width = deviations * sqrt(s[1])
    height = deviations * sqrt(s[2])

    height &lt; width || throw(ErrorException(&quot;width must be greater than height&quot;))

    return (orientation, width, height)
end

end
</code></pre>
